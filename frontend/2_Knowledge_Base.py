import os
import time
import pandas as pd
import streamlit as st

from frontend.sidebar import footer
from frontend.state import init_keys
from server.utils.file import save_uploaded_file, get_save_dir

def handle_file():
   
    st.subheader(
        "ä¸Šä¼ PDFï¼ŒDOCXï¼ŒTXTç­‰æ–‡ä»¶",
        help="Upload files to create a knowledge base index.",
        )
    
    with st.form("my-form", clear_on_submit=True):
        st.session_state.selected_files = st.file_uploader("ä¸Šä¼ æ–‡ä»¶ï¼š", accept_multiple_files=True, label_visibility="hidden")
        submitted = st.form_submit_button(
            "ä¸Šä¼ æ–‡ä»¶",
            help="é€‰æ‹©æ–‡ä»¶åç‚¹è¿™é‡Œä¸Šä¼ ",
            )
        if len(st.session_state.selected_files) > 0 and submitted:
            print("å¼€å§‹ä¸Šä¼ æ–‡ä»¶...")
            print(st.session_state.selected_files)
            for selected_file in st.session_state.selected_files:
                with st.spinner(f"æ­£åœ¨ä¸Šä¼  {selected_file.name}..."):
                    save_dir = get_save_dir()
                    save_uploaded_file(selected_file, save_dir)
                    st.session_state.uploaded_files.append({"name": selected_file.name, "type": selected_file.type, "size": selected_file.size})
            st.toast('âœ”ï¸ æ–‡ä»¶å·²ä¸Šä¼ ', icon='ğŸ‰')

    if len(st.session_state.uploaded_files) > 0:
        with st.expander(
                "å·²ä¸Šä¼ æ–‡ä»¶",
                expanded=True,
        ):
            df = pd.DataFrame(st.session_state.uploaded_files)
            st.dataframe(
                df,
                column_config={
                    "name": "æ–‡ä»¶å",
                    "size": st.column_config.NumberColumn(
                        "å¤§å°", format="%d å­—èŠ‚",
                    ),
                    "type": "ç±»å‹",
                },
                hide_index=True,
            )

    with st.expander(
            "æ–‡æœ¬å¤„ç†å‚æ•°é…ç½®",
            expanded=True,
    ):
        cols = st.columns(3)
        chunk_size = cols[0].number_input("å•æ®µæ–‡æœ¬æœ€å¤§é•¿åº¦ï¼š", 1, 4096, st.session_state.chunk_size)
        chunk_overlap = cols[1].number_input("ç›¸é‚»æ–‡æœ¬é‡åˆé•¿åº¦ï¼š", 0, st.session_state.chunk_size, st.session_state.chunk_overlap)
        cols[2].write("")
        cols[2].write("")
        zh_title_enhance = cols[2].checkbox("å¼€å¯ä¸­æ–‡æ ‡é¢˜åŠ å¼º", st.session_state.zh_title_enhance)

    if st.button(
        "ç”Ÿæˆç´¢å¼•",
        disabled=len(st.session_state.uploaded_files) == 0,
        help="ä¸Šä¼ æ–‡ä»¶åç‚¹è¿™é‡Œç”Ÿæˆç´¢å¼•ï¼Œä¿å­˜åˆ°çŸ¥è¯†åº“ä¸­",
    ):
        print("æ­£åœ¨ç”Ÿæˆç´¢å¼•...")
        with st.spinner(text="åŠ è½½æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•ï¼Œéœ€è¦1-2åˆ†é’Ÿ"):
            st.session_state.index_manager.load_files(st.session_state.uploaded_files, chunk_size, chunk_overlap, zh_title_enhance)
            st.toast('âœ”ï¸ çŸ¥è¯†åº“ç´¢å¼•ç”Ÿæˆå®Œæ¯•', icon='ğŸ‰')
            st.session_state.uploaded_files = []
            time.sleep(4)
            st.rerun()

def handle_website():
    st.subheader(
        "ç½‘é¡µä¿¡æ¯å¤„ç†",
        help="Enter a list of URLs to extract text and metadata from web pages.",
        )

    with st.form("website-form", clear_on_submit=True):

        col1, col2 = st.columns([1, 0.2])
        with col1:
            new_website = st.text_input("è¯·è¾“å…¥ç½‘é¡µåœ°å€", label_visibility="collapsed")
        with col2:
            add_button = st.form_submit_button("æ·»åŠ ")
            if add_button and new_website != "":
                st.session_state["websites"].append(new_website)

    if  st.session_state["websites"] != []:
        st.markdown(f"<p>Website(s)</p>", unsafe_allow_html=True)
        for site in  st.session_state["websites"]:
            st.caption(f"- {site}")
        st.write("")

    with st.expander(
            "æ–‡æœ¬å¤„ç†å‚æ•°é…ç½®",
            expanded=True,
    ):
        cols = st.columns(3)
        chunk_size = cols[0].number_input("å•æ®µæ–‡æœ¬æœ€å¤§é•¿åº¦ï¼š", 1, 4096, st.session_state.chunk_size, key="web_chunk_size")
        chunk_overlap = cols[1].number_input("ç›¸é‚»æ–‡æœ¬é‡åˆé•¿åº¦ï¼š", 0, st.session_state.chunk_size, st.session_state.chunk_overlap, key="web_chunk_overlap")
        cols[2].write("")
        cols[2].write("")
        zh_title_enhance = cols[2].checkbox("å¼€å¯ä¸­æ–‡æ ‡é¢˜åŠ å¼º", st.session_state.zh_title_enhance, key="web_zh_title_enhance")


    process_button = st.button("ç”Ÿæˆç´¢å¼•", 
                                key="process_website",
                                disabled=len(st.session_state["websites"]) == 0)
    if process_button:
        print("æ­£åœ¨ç”Ÿæˆç´¢å¼•...")
        with st.spinner(text="åŠ è½½æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•ï¼Œéœ€è¦1-2åˆ†é’Ÿ"):
            st.session_state.index_manager.load_websites(st.session_state["websites"], chunk_size, chunk_overlap, zh_title_enhance)
            st.toast('âœ”ï¸ çŸ¥è¯†åº“ç´¢å¼•ç”Ÿæˆå®Œæ¯•', icon='ğŸ‰')
            st.session_state.websites = []
            time.sleep(4)
            st.rerun()

def get_unique_files_info(ref_doc_info):
    unique_files = []
    seen_paths = set()

    for ref_doc in ref_doc_info.values():
        metadata = ref_doc.metadata
        file_path = metadata.get('file_path', None)

        if file_path is None:
            print(f"File path not found in ref doc: {ref_doc}") 
            # TODO: website ref docä¸­metadataä¸º{}

        if file_path and file_path not in seen_paths:
            file_info = {
                'file_name': metadata['file_name'],
                'file_path': file_path,
                'file_type': metadata['file_type'],
                'file_size': metadata['file_size'],
                'creation_date': metadata['creation_date']
            }
            unique_files.append(file_info)
            seen_paths.add(file_path)

    return unique_files


def handle_knowledgebase():
    st.subheader(
        "çŸ¥è¯†åº“å†…å®¹ç®¡ç†",
        help="View and manage the knowledge base index.",
        )
        
    from server.stores.strage_context import STORAGE_CONTEXT
    doc_store = STORAGE_CONTEXT.docstore
    if len(doc_store.docs) > 0:
        ref_doc_info = doc_store.get_all_ref_doc_info()
        unique_files= get_unique_files_info(ref_doc_info)
        st.write(f"æ€»æ•°ï¼š{len(unique_files)}")
        df = pd.DataFrame(unique_files)
        st.dataframe(
            df,
            column_config={
                "file_name": "åç§°",
                "file_path": "è·¯å¾„",
                "file_type": "ç±»å‹",
                "file_size": st.column_config.NumberColumn(
                    "å¤§å°", format="%d å­—èŠ‚",
                ),
                "creation_date": "åˆ›å»ºæ—¥æœŸ",
            },
            hide_index=True,
        )
    else:
        st.write("çŸ¥è¯†åº“ä¸­æ²¡æœ‰å†…å®¹")

def main():
    st.header("çŸ¥è¯†åº“")
    st.caption("ç®¡ç†çŸ¥è¯†åº“å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡ä»¶ã€ç½‘é¡µç­‰")

    tab1, tab2, tab3 = st.tabs(["æ·»åŠ æ–‡ä»¶", "æ·»åŠ ç½‘å€", "çŸ¥è¯†åº“ç®¡ç†"])

    with tab1:
        handle_file()

    with tab2:
        handle_website()

    with tab3:
        handle_knowledgebase()


init_keys()
footer()

main()


