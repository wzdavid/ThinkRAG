import streamlit as st
import server.models.ollama as ollama
from frontend.sidebar import footer
from frontend.state import init_keys
from config import EMBEDDING_MODEL_PATH, RERANKER_MODEL_PATH, RERANKER_MODEL_TOP_N

def change_ollama_endpoint():
    st.session_state.ollama_api_url = st.session_state.ollama_endpoint

def settings():
    st.header("è®¾ç½®")
    st.caption("è®¾ç½®æ¨¡å‹ä¸å‚æ•°")

    st.subheader(
        "Ollama",
        help="Ollama is a lightweight, fast, and scalable large language model (LLM) server that provides a RESTful API for interacting with LLMs. Ollama can run on a single machine or in a distributed environment, and can be easily deployed on-premises or in the cloud.",
        )
    chat_settings = st.container(border=True)
    with chat_settings:
        st.text_input(
            "è®¾ç½®Ollama APIæœåŠ¡åœ°å€",
            key="ollama_endpoint",
            value=st.session_state.ollama_api_url,
            on_change=change_ollama_endpoint,
        )
        if ollama.is_alive():
            ollama.get_model_list()
            st.write("ğŸŸ¢ Ollamaè¿è¡Œä¸­")
            with st.expander("å½“å‰å¯ç”¨çš„æ¨¡å‹"):
                st.write(st.session_state.ollama_models)
        else:
            st.write("ğŸ”´ Ollamaæœªè¿è¡Œ")

        st.button(
            "åˆ·æ–°",
            on_click=ollama.get_model_list,
        )

    st.subheader(
        "å¤§æ¨¡å‹API",
        help="Large language models (LLMs) are powerful models that can generate human-like text based on the input they receive. LLMs can be used for a wide range of natural language processing tasks, including text generation, question answering, and summarization.",
    )
    llm_api_settings = st.container(border=True)
    with llm_api_settings:
        with st.expander("å½“å‰å¯ç”¨çš„æ¨¡å‹"):
            st.write(st.session_state.llm_api_list)

    st.subheader(
        "åµŒå…¥æ¨¡å‹",
        help="Embeddings are numerical representations of data, useful for tasks like document clustering and similarity detection when processing files, as they encode semantic meaning for efficient manipulation and retrieval.",
    )
    embedding_settings = st.container(border=True)
    with embedding_settings:
        embedding_model_list = list(EMBEDDING_MODEL_PATH.keys())
        embedding_model = st.selectbox(
            "åµŒå…¥æ¨¡å‹", 
            embedding_model_list,
            key="selected_embedding_model",
            index=embedding_model_list.index(st.session_state["embedding_model"]),
        )
        st.session_state["embedding_model"] = embedding_model

        st.toggle("å¯ç”¨é‡æ’", key="selected_use_reranker", value= st.session_state.use_reranker) # é»˜è®¤å…³é—­
        st.session_state.use_reranker = st.session_state["selected_use_reranker"]
        if st.session_state.use_reranker == True:
            reranker_model_list = list(RERANKER_MODEL_PATH.keys())
            reranker_model = st.selectbox(
                "é‡æ’æ¨¡å‹", 
                reranker_model_list,
                key="selected_reranker_model",
                index=reranker_model_list.index(st.session_state["reranker_model"]),
            )
            st.session_state["reranker_model"] = reranker_model

    st.toggle("æ˜¾ç¤ºé«˜çº§è®¾ç½®", key="advanced", value= False) # é»˜è®¤å…³é—­

    if st.session_state["advanced"] == True:
        st.subheader("é«˜çº§è®¾ç½®")
        advanced_settings = st.container(border=True)
        with advanced_settings:
            st.select_slider(
                "Top K",
                options=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                help="The number of most similar documents to retrieve in response to a query.",
                value=st.session_state["top_k"],
                key="top_k",
            )
            st.text_area(
                "System Prompt",
                value=st.session_state["system_prompt"],
                key="system_prompt",
            )
            st.selectbox(
                "Chat Mode",
                (
                    "compact",
                    "refine",
                    "tree_summarize",
                    "simple_summarize",
                    "accumulate",
                    "compact_accumulate",
                ),
                help="Sets the [Llama Index Query Engine chat mode](https://github.com/run-llama/llama_index/blob/main/docs/module_guides/deploying/query_engine/response_modes.md) used when creating the Query Engine. Default: `compact`.",
                key="chat_mode",
                disabled=True,
            )
            st.write("")
        with st.expander("å½“å‰åº”ç”¨å‚æ•°åˆ—è¡¨"):
            state = dict(sorted(st.session_state.items()))
            st.write(state)

init_keys()

footer()

settings()